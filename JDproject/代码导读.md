### 1、爬取网站

初始url：`https://book.jd.com/booksort.html`

网站内容是通过js进行渲染的，用原始的scrapy是爬取不了的，所以在这里需要借助selenium作为中间键进行页面的渲染

### 2、爬虫代码

这下面这些是scrapy-redis的基本写法

```python
name = 'JD'  # 爬虫名字
redis_key = 'jd'  # 起始url的key名字，用lpush key url存入数据
allowed_domains = ['jd.com']

def __init__(self, *args, **kwargs):
    # Dynamically define the allowed domains list.
    domain = kwargs.pop('domain', '')
    self.allowed_domains = list(filter(None, domain.split(',')))
    super(JdSpider, self).__init__(*args, **kwargs)
```

这下面的代码是解析出初始页面中每个系列的名称和url，再通过url进行在一部的页面请求，但该页面需要进行selenium渲染，否则提取不到信息
meta是传过去data_spider的数据，必须是字典类型的

```python
    # 解析出类标签的url
    def parse(self, response):
        dt = response.xpath('//*[@id="booksort"]/div[2]/dl/dt')
        for big in dt:
            # 查询同级下的下一个标签
            s_title = big.xpath('./following-sibling::dd[1]/em')
            for s in s_title:
                temp = {}
                temp["small_name"] = s.xpath('./a/text()').extract_first()
                temp["small_url"] = "https:" + s.xpath('./a/@href').extract_first()
                yield scrapy.Request(url=temp["small_url"],
                                     callback=self.data_spider,
                                     meta={"meta": temp}
                                     )
```

这下面的代码部分是将页面信息进行数据提取，由于页面的加载未能一次加载完，需要进行页面的滑动，所以需要selenium进行页面的滑动渲染，在中间件

```python
# 提取各类里的书籍名称
def data_spider(self, response):
    temp = response.meta["meta"]
    book_list = response.xpath('//*[@id="J_goodsList"]/ul/li')
    for book in book_list:
        item = JdprojectItem()
        item["small_name"] = temp["small_name"]
        item["small_url"] = temp["small_url"]

        item["book_name"] = book.xpath('./div/div[3]/a/em/text()').extract_first()
        item["book_url"] = "https:" + book.xpath('./div/div[1]/a/@href').extract_first()
        # 判断错误的url信息进行去除
        if "v=404" in item["book_url"]:
            continue
        item["price"] = book.xpath('./div/div[2]/strong/i/text()').extract_first()
        item["press"] = book.xpath('./div/div[6]/a/text()').extract_first()
        yield item
```

### 3、中间件middlewares

下面是中间件的写法，主要是给页面进行滑动渲染操作的，这里返回的是response对象，他不会进入下载中间件，会直接通过引擎传给爬虫

```python
from selenium import webdriver
import time
from scrapy.http import HtmlResponse

# useful for handling different item types with a single interface
from itemadapter import is_item, ItemAdapter


# 使用selenium进行页面渲染
class SeleniumMiddleware(object):
    def process_request(self, request, spider):
        url = request.url  # 提取出url
        # 当页面是图书分类的页面时，使用下面方式进行页面渲染
        if "booksort" in url:
            # ===设置不加载页面
            options = webdriver.ChromeOptions()
            prefs = {
                'profile.default_content_setting_values': {
                    'images': 2,
                }
            }
            options.add_experimental_option('prefs', prefs)
            # ====
            driver = webdriver.Chrome(chrome_options=options)
            # 可以设置为无头模式
            driver.get(url)
            time.sleep(3)
            data = driver.page_source
            driver.close()

            # 创建响应对象，必须创建响应对象返回
            res = HtmlResponse(url=url, body=data, encoding='utf-8', request=request)
            return res  # 当放回的是Response时，他不会经过下载中间件
        # 当页面是图书详情页面时，使用下面进行页面渲染，让页面向下滑
        if "list.jd" in url:
            # ===设置不加载页面
            options = webdriver.ChromeOptions()
            prefs = {
                'profile.default_content_setting_values': {
                    'images': 2,
                }
            }
            options.add_experimental_option('prefs', prefs)

            options.add_argument('--headless')  # 设置无界面

            # ====
            driver = webdriver.Chrome(chrome_options=options)
            driver.maximize_window()  # 页面最大化
            driver.get(url)
            time.sleep(3)
            driver.execute_script('window.scrollBy(0,4000)')
            time.sleep(1)
            driver.execute_script('window.scrollBy(0,4000)')
            time.sleep(1)
            data = driver.page_source

            driver.close()

            # 创建响应对象，必须创建响应对象返回
            res = HtmlResponse(url=url, body=data, encoding='utf-8', request=request)
            return res  # 当放回的是Response时，他不会经过下载中间件
```

### 4、配置文件settings

```python
# Scrapy settings for example project
#
# For simplicity, this file contains only the most important settings by
# default. All the other settings are documented here:
#
#     http://doc.scrapy.org/topics/settings.html

SPIDER_MODULES = ['JD.spiders']
NEWSPIDER_MODULE = 'JD.spiders'

# 替换成自己的UA
USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.146 ' \
             'Safari/537.36 '
# 设置重复过滤器的模块
DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"
# 设置调度器，scrapy_resis中的调度器具备与数据库交互的功能
SCHEDULER = "scrapy_redis.scheduler.Scheduler"
# 设置爬虫结束的时候是否保存redis数据库的去重集合与任务队列
SCHEDULER_PERSIST = True
# SCHEDULER_QUEUE_CLASS = "scrapy_redis.queue.SpiderPriorityQueue"
# SCHEDULER_QUEUE_CLASS = "scrapy_redis.queue.SpiderQueue"
# SCHEDULER_QUEUE_CLASS = "scrapy_redis.queue.SpiderStack"

# 这里如果用不需要的可以注释掉
ITEM_PIPELINES = {
    # 'example.pipelines.ExamplePipeline': 300,
    'scrapy_redis.pipelines.RedisPipeline': 400,
}

# 设置redis数据库
REDIS_URL = "redis://127.0.0.1:6379"

# LOG_LEVEL = 'DEBUG'  # 可以注释掉

# Introduce an artifical delay to make use of parallelism. to speed up the
# crawl.
DOWNLOAD_DELAY = 1  # 这个是每个爬虫间隔的时间

# 中间件开启
DOWNLOADER_MIDDLEWARES = {
    # 'JDproject.middlewares.JdprojectDownloaderMiddleware': 543,
    'JDproject.middlewares.SeleniumMiddleware': 543,

}
```

### 5、代码执行

代码执行终端进入到爬虫文件的目录，执行scrapy runspider JD.py domain='jd.com'
然后给redis传入初始url：lpush jd https://book.jd.com/booksort.html